---
layout: archive
title: "Retrieval Augmentation"
permalink: /readings/rag/
author_profile: false
sidebar: toc
redirect_from:
  - /readings/rag.html
---

---
### [Memory-Augmented LLM Personalization with Short- and Long-Term Memory Coordination](https://arxiv.org/pdf/2309.11696.pdf) (2023)

> TBC

---
### [ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation](https://arxiv.org/pdf/2308.11131.pdf) (2023)

> TBC

---
### [RUEL: Retrieval-Augmented User Representation with Edge Browser Logs for Sequential Recommendation](https://paperswithcode.com/paper/ruel-retrieval-augmented-user-representation) (2023)

> we propose RUEL, a novel retrieval-based sequential recommender that can effectively incorporate external anonymous user behavior data.
...
We then design a contrastive learning framework with a momentum encoder and a memory bank to retrieve the most relevant and diverse browsing sequences from the full browsing log based on the semantic similarity between user representations. After retrieval, we apply an item-level attentive selector to filter out noisy items and generate refined sequence embeddings for the final predictor. 

> TBC

---
### [Atlas: Few-shot Learning with Retrieval Augmented Language Models](https://arxiv.org/pdf/2208.03299.pdf) (2022)

> TBC


---
### [RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models](https://arxiv.org/pdf/2308.07922.pdf) (2023)

> We
find that [ATLAS](#atlas-few-shot-learning-with-retrieval-augmented-language-models-2022) exhibits a certain in-context learning ability; however, due to a mismatch between
pretraining and testing and a limited context length—issues that are common to existing encoderdecoder LMs trained with masked language modeling—its few-shot performance is not stable and
providing more than, e.g., 8-shot, examples does not lead to further improvement.

> While there is growing interest in this area, most studies have focused on incontext learning with decoder-only LMs, e.g., GPT-3 (Brown et al., 2020). However, bidirectional
LMs like BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020) have been shown to achieve
superior performance on various natural language understanding tasks, indicating that they may
offer unique advantages for in-context learning as well.
...
 For instance, Patel
et al. (2023) demonstrate that bidirectional models can outperform decoder-only LMs of a similar
scale regarding in-context learning; however, there is still a significant performance gap compared to
decoder-only models on a much larger scale.

> While there
has been some research on in-context learning with retrieval-augmented decoder-only LMs, which
can be straightforwardly implemented by concatenating retrieved passages with the query as the
input of the LM (Mallen et al., 2022; Shi et al., 2023; Khattab et al., 2022), in-context learning with
retrieval-augmented encoder-decoder LMs, such as ATLAS, remains unexplored
> TBC

---
### [RETRIEVAL MEETS LONG CONTEXT LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2310.03025.pdf) (2023)

> we find that LLM
with 4K context window using simple retrieval-augmentation at generation can
achieve comparable performance to finetuned LLM with 16K context window via
positional interpolation on long context tasks, while taking much less computation.
More importantly, we demonstrate that retrieval can significantly improve the
performance of LLMs regardless of their extended context window sizes

> TBC


---
### [MAKING RETRIEVAL-AUGMENTED LANGUAGE MODELS ROBUST TO IRRELEVANT CONTEXT](https://arxiv.org/pdf/2310.01558v1.pdf) (2023)

> TBC

---
### [UNDERSTANDING RETRIEVAL AUGMENTATION FOR LONG-FORM QUESTION ANSWERING](https://arxiv.org/pdf/2310.12150.pdf) (2023)

> TBC


---
### [RET-LLM: Towards a General Read-Write Memory for Large Language Models](https://arxiv.org/pdf/2305.14322.pdf) (2023)

> TBC


---
### [Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study](https://arxiv.org/pdf/2304.06762.pdf) (2023)

> TBC


---
### [In-Context Retrieval-Augmented Language Models](https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/63c6c20dec4479564db21819_NEW_In_Context_Retrieval_Augmented_Language_Models.pdf) (2023)

> TBC


---
### [RecMind: Large Language Model Powered Agent For Recommendation](https://arxiv.org/pdf/2308.14296.pdf) (2023)

> TBC


---
### [Generative Slate Recommendation with Reinforcement Learning](https://arxiv.org/pdf/2301.08632.pdf) (2023)

> TBC



---
### [LlamaRec: Two-Stage Recommendation using Large Language Models for Ranking](https://arxiv.org/pdf/2311.02089.pdf) (2023)

> TBC


---
### [Integrating Summarization and Retrieval for Enhanced Personalization via Large Language Models](https://arxiv.org/pdf/2310.20081v1.pdf) (2023)

> TBC


<!-- ---
### []() ()

> TBC-->

<!-- ---
### []() ()

> TBC-->

<!-- ---
### []() ()

> TBC-->

