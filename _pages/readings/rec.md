---
layout: archive
title: " Sentence Transformer"
permalink: /readings/rec/
author_profile: false
sidebar: toc
redirect_from:
  - /readings/rec.html
---


---
### [PALR: Personalization Aware LLMs for Recommendation](https://arxiv.org/pdf/2305.07622.pdf) (2023 - short)

> we first use user/item
interactions as guidance for candidate retrieval, and then adopt an
LLM-based ranking model to generate recommended items.
... we fine-tune an LLM of 7 billion parameters
for the ranking purpose. This model takes retrieval candidates in
natural language format as input, with instructions explicitly asking
to select results from input candidates during inference. 

> Initially, we use an LLM and user
behavior as input to generate user profile keywords. Then, we
employ a retrieval module to pre-filter some candidates from the
items pool based on the user profile.
... Finally, we use LLM
to provide recommendations from those candidates based on user
history behaviors. To better adapt these general-purpose LLMs to
fit the recommendation scenarios, we convert user behavior data
into natural language prompts and fine-tune a LLaMa[22] 7B model.
Our goal is to teach the model to learn the co-occurrence of user
engaged item patterns. This approach enables us to incorporate user
behavior data into the LLM’ reasoning process and better generalize
to new users and unseen items. 

> An LLM can be
leveraged to generate a summary of a user’s preferences.
The "Natural Language Prompt" for the "LLM for
recommendation" comprises three components: the "Interaction History Sequence," the "Natural Language User Profile,"
and the "Candidates for Recommendation". The "Interaction History Sequence" is created by simply concatenating
the items that the user has interacted with. The "Natural
Language User Profile" is a high-level summarization of the
user’s preferences, generated using an LLM based on useritem interactions, item attributes, or even user information
if possible. The "Candidates for Recommendation" are the
output of a retrieval model, and in our design, we have the
flexibility to use any retrieval model for this purpose.

Finetune:
> The "Recommend" task involves a list of items that the user has interacted with in the past
(with a maximum limit of 20 items), and the objective of the model
is to generate a list of "future" items that the user may interact with.
We refer to a model fine-tuned by this instruction as 𝑃𝐴𝐿𝑅𝑣1.

> The "Recommend_Retrieval" task asks the model to retrieve the
target "future" items from a list of candidate items. The candidate
list contains all target items, plus a few negative items similar to the
target items (e.g. movies with the same genres, co-watched by many
users). We refer to a model fine-tuned with both "Recommend"
and "Recommend_Retrieval" instruction as 𝑃𝐴𝐿𝑅𝑣2.

> In this paper, we utilize
SASRec as our retrieval layer and consider its top 50 recommendations. By comparing 𝑃𝐴𝐿𝑅𝑣2 and SASRec, it’s obvious that the
top10 recommendations re-ranked by our PALR are superior to
the original recommendations provided by SASRec.

> We could observe
𝑃𝐴𝐿𝑅𝑣1 has shown some ability to connect historical interacted
items with possible future interacted items. Prior to fine-tuning, the
model tends to only recommend popular movies in movie recommendation tasks. However, 𝑃𝐴𝐿𝑅𝑣1 isn’t able to retrieve the target
item from a list of candidate items. We have tried to use 𝑃𝐴𝐿𝑅𝑣1
for retrieval and observe that it could only randomly select from
the candidates. The performance from 𝑃𝐴𝐿𝑅𝑣2 has demonstrated
the effectiveness of incorporating an additional instruction during
the fine-tuning stage.

---
### [Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models](https://arxiv.org/pdf/2306.10933.pdf) (2023)


---
### [LLM-Rec: Personalized Recommendation via Prompting Large Language Models](https://arxiv.org/pdf/2307.15780.pdf) (2023)

> Our empirical experiments show that incorporating the augmented input text generated by LLM leads to improved recommendation performance. Recommendation-driven and engagement-guided prompting strategies
are found to elicit LLM’s understanding of global and local item characteristics.

> Rather than using LLMs as
recommender models, this study delves into the exploration of prompting strategies to augment input
text with LLMs for personalized content recommendation. 

> To create the engagement-guided prompt, we combine the content description of the target item,
denoted as dtarget, with the content descriptions of T important neighbor items, represented as
d1, d2, · · · , dT . The importance is measured based on user engagement. We will discuss more
details in the Experiment section. This fusion of information forms the basis of the prompt, which
is designed to leverage user engagement and preferences in generating more contextually relevant
content descriptions: “Summarize the commonalities among the following descriptions: ‘dtarget’; ‘d1; d2; ... dT ’.”

> Item module. Text encoder: We use Sentence-BERT [14] to derive the textual embeddings from the original content description and augmented text. The embedding model is all-MiniLM-L6-v2.

> Importance measurement for engagement-guided prompting: In our study, we show
an example of use Personalized PageRank (PPR) score as the importance measurement.
... The rationale
behind this approach lies in the observation that when users frequently engage with two
items, there tends to be a greater similarity in terms of user preferences. 

> LLM-Rec
boosts simple MLP models to achieve superior recommendation performance, surpassing other more
complex feature-based recommendation methods.

<!-- ---
### []() () -->


<!-- ---
### []() () -->
<!-- ---
### []() () -->
<!-- ---
### []() () -->
<!-- ---
### []() () -->
<!-- ---
### []() () -->
<!-- ---
### []() () -->