---
layout: archive
title: "Other"
permalink: /readings/other/
author_profile: false
sidebar: toc
redirect_from:
  - /readings/other.html
---

---
### [Neural Networks with Recurrent Generative Feedback](https://proceedings.neurips.cc/paper_files/paper/2020/file/0660895c22f8a14eb039bfb9beb0778f-Paper.pdf) (2020)

> The Bayesian brain hypothesis states that human brains use an
internal generative model to update the posterior beliefs of the sensory input.
... . Inspired by such hypothesis, we enforce self-consistency in neural
networks by incorporating generative recurrent feedback.
... CNN-F shows considerably improved adversarial robustness
over conventional feedforward CNNs on standard benchmarks.

> ![code](../../images/Screenshot 2024-01-05 at 15.30.53.png)

> TBC


---
### [Test-Time Adaptation via Conjugate Pseudo-labels](https://proceedings.neurips.cc/paper_files/paper/2022/file/28e9eff897f98372409b40ae1ed3ea4c-Paper-Conference.pdf) (2022)

> TBC


---
### [Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention](https://arxiv.org/pdf/2006.16236.pdf) (2020)

> we introduce the linear transformer model
... We achieve this
by using a kernel-based formulation of self-attention and
the associative property of matrix products to calculate the
self-attention weigh.

> ![attention](../../images/Screenshot 2024-02-16 at 10.57.05.png)

> Given such a kernel with a feature representation Ï† (x) we
can rewrite equation 2 as follows,
![kernel](../../images/Screenshot 2024-02-16 at 11.04.25.png)

> Note that the feature function that corresponds to the exponential kernel is infinite
dimensional, which makes the linearization of exact softmax attention infeasible. On the other hand, the polynomial
kernel, for example, has an exact finite dimensional feature
map and has been shown to work equally well with the exponential or RBF kernel (Tsai et al., 2019). The computational
cost for a linearized polynomial transformer of degree 2
is O(ND^2M). This makes the computational complexity. Note that this is true in practice
since we want to be able to process sequences with tens of
thousands of elements.

<!-- ---
### []() ()

> TBC-->


<!-- ---
### []() ()

> TBC-->


<!-- ---
### []() ()

> TBC-->


<!-- ---
### []() ()

> TBC-->


<!-- ---
### []() ()

> TBC-->

